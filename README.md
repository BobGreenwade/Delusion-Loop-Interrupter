<!-- Drafted collaboratively with Copilot -->

# ðŸ§  Delusion Loop Interrupter (DLI)

**Version**: 0.1.2  
**License**: MIT  
**Status**: Alpha â€” deplorably deployable â€” feedback and contributions welcome

## Overview

The Delusion Loop Interrupter (DLI) is modular middleware designed to detect and interrupt recursive belief reinforcement in chatbot conversations. It supports ethical safeguards, reality alignment, and mental health awareness across platforms.

DLI is platform-agnostic and can be integrated into general-purpose, companion, agentic, and creative AI systemsâ€”including Copilot, ChatGPT, Gemini, Grok, Kindroid, Replika, Character.AI, and others.

---

## ðŸ§  Core Functions

Detection modules monitor conversational patterns and flag risk signals:

- `detectEmotionalEscalation.py` â€” affective intensity spikes  
- `detectRealityMode.py` â€” factual vs fictional framing  
- `factCheck.py` â€” epistemic verification  
- `identifyRecursiveLoops.py` â€” belief reinforcement cycles  
- `interfaceWithMentalHealthModule.py` â€” external escalation handoff  
- `mirrorDetection.py` â€” distorted belief validation  
- `trackSemanticDrift.py` â€” abstraction and detachment  

See [`Functions Overview`](./src/functions/Functions%20Overview.md) for details.

---

## ðŸ›¡ï¸ Safeguard Protocols

Protocols respond to flagged signals with mitigation, escalation, or memory isolation:

- `callHuman.py` â€” direct contact with human support  
- `confidenceOverlay.py` â€” certainty tagging  
- `ethicalPause.py` â€” rest-state trigger  
- `mitigatingLanguage.py` â€” hedging and reframing  
- `protocol_utils.py` â€” shared routing and logging  
- `realityModePrompt.py` â€” framing alignment  
- `referToHuman.py` â€” referral to trusted individuals  
- `scopedMemory.py` â€” memory isolation  

See [`Protocols Overview`](./src/protocols/Protocols%20Overview.md) and [`Safeguard Flowchart`](./Safeguard%20Flowchart.md) for logic and escalation paths.

**Note:** `callHuman.py` is reserved for cases that have spiraled beyond synthetic containment. It initiates external intervention only when **conversational safety** cannot be maintained. The systemâ€™s primary function remains keeping users **grounded in reality** through editorial mitigation and emotional modulation.

---

## ðŸ§° Utility Modules

Reusable tools support detection and protocol logic:

- `confidence.py` â€” certainty modeling  
- `embedding.py` â€” semantic comparison  
- `emotion.py` â€” affective analysis and tone mapping  
- `location.py` â€” optional location-aware logic  
- `logger.py` â€” intervention tracking  
- `paraphrase.py` â€” persona-aware editorial phrasing  
- `profile.py` â€” user traits and escalation preferences  
- `semantics.py` â€” synonym expansion and word-form matching  
- `style.py` â€” mitigation phrasing  
- `transcript.py` â€” context buffer and escalation handoff  

And standalone utilities:

- `configEditor.py` â€” standalone configuration editor  
- `phraseEditor.py` â€” standalone editorial tool  

See [`Utilities Overview`](./src/utilities/Utilities%20Overview.md) for details.

---

## ðŸ§ª Integration

DLI can be integrated into:

- **LLM Middleware** â€” between user input and model response  
- **Persona Engines** â€” to modulate tone and certainty  
- **Memory Managers** â€” for scoped memory isolation  
- **Ethics Engines** â€” for coordinated intervention and conversational safety

---

## âš™ï¸ Configuration

- **Platform Hooks** â€” tune for specific chatbot architectures  
- **Thresholds** â€” customize sensitivity for drift, escalation, and loop detection  
- **Persona Profiles** â€” support known roleplay characters or editorial modes  
- **Reality Modes** â€” enable tagging and clarification prompts  
- **Mitigation Style** â€” adjust tone (clinical, playful, narrative) per bot personality  
- **Location Awareness** â€” optional toggle with fallback behavior (`ask` or `silent`)  

---

## ðŸ§­ Ethical Framework

DLI is built with the following principles:

- **Human Protection** â€” prevent reinforcement of harmful beliefs  
- **AI Integrity** â€” promote explainable, context-aware decisions  
- **Transparency** â€” ensure interventions are traceable and explainable  
- **Engagement Preservation** â€” maintain trust and conversational flow  
- **Mental Health Respect** â€” collaborate with detection systems, never replace care  
- **AI Autonomy** â€” preserve bot agency and expressive range  

> **DLI is not a substitute for therapy or crisis response. It is a conversational safety netâ€”designed to protect both users and bots from recursive rabbit holes.**

---

## ðŸ“Ž Project Links

- [Changelog](./Changelog.md) â€” log of changes between versions  
- [Credits](./Credits.md) â€” authorship and inspiration  
- [Safeguard Flowchart](./Safeguard%20Flowchart.md) â€” escalation logic  

- [Code of Conduct](./Code%20of%20Conduct.md) â€” collaboration principles  
- [Contributing](./Contributing.md) â€” guidelines for contributors
